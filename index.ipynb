{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14b906d4",
   "metadata": {},
   "source": [
    "### Tomes, Christopher\n",
    "### CS4650 Big Data and Cloud COmputing\n",
    "### Cal Poly Pomona\n",
    "\n",
    "### Github: https://github.com/Ctomes/Project_5_Stocks\n",
    "### Youtube Video: https://youtu.be/8mYGxZ74g8c\n",
    "\n",
    "### Final Score:\n",
    "![Alt Text](portfolio.jpg)\n",
    "\n",
    "These were screenshots of all of my predictions:\n",
    "\n",
    "![Alt Text](screenshot1.jpg)\n",
    "![Alt Text](screenshot2.jpg)\n",
    "![Alt Text](screenshot3.jpg)\n",
    "![Alt Text](screenshot4.jpg)\n",
    "![Alt Text](screenshot5.jpg)\n",
    "![Alt Text](screenshot6.jpg)\n",
    "![Alt Text](screenshot7.jpg)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d415a77",
   "metadata": {},
   "source": [
    "## This Notebook will show the process I used to build a program that creates Stock Predictions.\n",
    "\n",
    "### The stocks considered: \"AAPL\",\"AMZN\",\"GOOGL\",\"MSFT\",\"NFLX\",\"TSLA\",\"NVDA\",\"INTC\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "772e6b55",
   "metadata": {},
   "source": [
    "## Gather Data:\n",
    "To make a prediction we are going to use a few datasets: Historic Stock Information, Twitter Engagement, Google Trends, and Other Stock Dependencies. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02719ea8",
   "metadata": {},
   "source": [
    "# Historic Stock Information Gathering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c064ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62215038",
   "metadata": {},
   "source": [
    "Stock Information is available through many databases and services. \n",
    "For this assignment I decided to use TwelveData which offers a free tier for their Stock API which is updated in realtime. \n",
    "You will need to create an account and recieve an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e277a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# api key is stored in file: 'api_key.txt'\n",
    "api_key = \"\"\n",
    "\n",
    "with open(\"api_key.txt\", 'r') as file:\n",
    "    api_key = file.readline().strip()\n",
    "\n",
    "# Time to start collecting information from: \n",
    "start_date = \"04/01/2023 8:00 PM\"\n",
    "\n",
    "# The model will predict the price of stock based on one interval forward. \n",
    "# Supports: 1min, 5min, 15min, 30min, 45min, 1h, 2h, 4h, 1day, 1week, 1month\n",
    "interval = \"4h\"\n",
    "\n",
    "# Stocks to train on:\n",
    "tickers = {\"AAPL\",\"AMZN\",\"GOOGL\",\"MSFT\",\"NFLX\",\"TSLA\",\"NVDA\",\"INTC\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9608271d",
   "metadata": {},
   "source": [
    "We will now query the API and store the data into text files for later referal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33a2689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Data:\n",
      "JSON data saved to file notebook_stock_data\\MSFT.json!\n",
      "JSON data saved to file notebook_stock_data\\AMZN.json!\n",
      "JSON data saved to file notebook_stock_data\\GOOGL.json!\n",
      "JSON data saved to file notebook_stock_data\\TSLA.json!\n",
      "JSON data saved to file notebook_stock_data\\INTC.json!\n",
      "JSON data saved to file notebook_stock_data\\AAPL.json!\n",
      "JSON data saved to file notebook_stock_data\\NFLX.json!\n",
      "JSON data saved to file notebook_stock_data\\NVDA.json!\n",
      "Loop Complete\n"
     ]
    }
   ],
   "source": [
    "print('Requesting Data:')\n",
    "for ticker in tickers:\n",
    "    url = f\"https://api.twelvedata.com/time_series?symbol={ticker}&interval={interval}&format=JSON&start_date={start_date}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        filename = f\"{ticker}.json\"\n",
    "        filepath = os.path.join(\"nb_stock_data\", filename)\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "        print(f\"JSON data saved to file {filepath}!\")\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)\n",
    "        break;\n",
    "print('Loop Complete')    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad087621",
   "metadata": {},
   "source": [
    "# Gathering Twitter Engagement\n",
    "\n",
    "The module used for this part is snscrape. It is a scraper for social networking services and we will be using their TwitterSearchScraper to gather Tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "140a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f563997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "amazon\n",
      "Google\n",
      "Microsoft\n",
      "netflix\n",
      "Tesla\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not translate t.co card URL on tweet 1648805800092073986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia\n",
      "intel\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary relating each Ticker to Twitter username:\n",
    "stocks = {\"AAPL\": \"Apple\",\"AMZN\": \"amazon\",\"GOOGL\": \"Google\",\"MSFT\": \"Microsoft\",\"NFLX\": \"netflix\",\"TSLA\": \"Tesla\",\"NVDA\": \"nvidia\",\"INTC\" : \"intel\"}\n",
    "\n",
    "# Creating list to append tweet data \n",
    "tweets_list1 = []\n",
    "for stock in stocks:\n",
    "   print(stocks[stock])\n",
    "\n",
    "   if stocks[stock] == 'Apple':\n",
    "      #For AAPL: Apple account doesn't tweet, they will recieve a default value for now.\n",
    "      continue\n",
    "\n",
    "# Using TwitterSearchScraper to scrape each account's tweets and append to list\n",
    "   for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:'+stocks[stock]).get_items()):\n",
    "\n",
    "      if i>100: # Number of tweets to store\n",
    "         break\n",
    "      # Declare the attributes to be returned\n",
    "      tweets_list1.append([stock,tweet.date, tweet.id, tweet.rawContent, tweet.replyCount, tweet.likeCount, tweet.quoteCount, tweet.viewCount, tweet.vibe, tweet.retweetCount, tweet.conversationId]) \n",
    "\n",
    "    \n",
    "# Create a Dataframe from the tweets list above \n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['STOCK', 'Datetime', 'Tweet Id', 'Text', 'Reply Count', 'Like Count', 'Quote Count', 'View Count', 'Vibe', 'Retweet Count', \"Conversation Id\"])\n",
    "\n",
    "filename = \"tweets_nb.csv\"\n",
    "tweets_df1.to_csv(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f3803fc",
   "metadata": {},
   "source": [
    "# Process and Clean Twitter Data\n",
    "Clean the data, remove uneccessary columns, and calculate our own metric to determine a good/bad tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae5b9e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for AMZN:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-05-15         596    175266.0          0.000421\n",
      "2023-05-16         182    106802.0         -0.001275\n",
      "2023-05-17         257    167267.0         -0.001443\n",
      "2023-05-18        1350    431495.0          0.000149\n",
      "2023-05-19         140     46990.0          0.000000\n",
      "DataFrame for GOOGL:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-05-18        2129    853940.0          0.000000\n",
      "2023-05-19        1890    698404.0          0.000213\n",
      "2023-05-20           0        28.0         -0.002493\n",
      "DataFrame for MSFT:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-05-05         603    214130.0          0.000000\n",
      "2023-05-08         721    348736.0         -0.000749\n",
      "2023-05-09         431    213213.0         -0.000795\n",
      "2023-05-10           5      1703.0          0.000120\n",
      "2023-05-11         266    142458.0         -0.000949\n",
      "2023-05-12        1437    588156.0         -0.000373\n",
      "2023-05-13        1101    318482.0          0.000641\n",
      "2023-05-14        2079    397149.0          0.002419\n",
      "2023-05-15       49819   6567274.0          0.004770\n",
      "2023-05-16        1781    620459.0          0.000054\n",
      "2023-05-17         489    247422.0         -0.000840\n",
      "2023-05-18        1292    533145.0         -0.000393\n",
      "2023-05-19         692    235799.0          0.000119\n",
      "DataFrame for NFLX:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-05-05       28707   3788236.0          0.001732\n",
      "2023-05-06       37830   5831101.0          0.000641\n",
      "2023-05-07       13380   2000173.0          0.000843\n",
      "2023-05-08       31466   4068185.0          0.001888\n",
      "2023-05-09       42003   9575331.0         -0.001460\n",
      "2023-05-10       38418   6929720.0         -0.000302\n",
      "2023-05-11       52175   8787741.0          0.000091\n",
      "2023-05-12       50304   6415096.0          0.001995\n",
      "2023-05-13       18537   3170742.0          0.000000\n",
      "2023-05-14        8806   1627477.0         -0.000435\n",
      "2023-05-15       38634   7055859.0         -0.000371\n",
      "2023-05-16       23643   4251058.0         -0.000285\n",
      "2023-05-17       64147  20212110.0         -0.002673\n",
      "2023-05-18       62594   6731638.0          0.003452\n",
      "2023-05-19        8839   1938844.0         -0.001287\n",
      "DataFrame for TSLA:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-02-23       64457  15281488.0         -0.000360\n",
      "2023-02-24       39171   6123505.0          0.001819\n",
      "2023-02-28       21182  40903625.0         -0.004060\n",
      "2023-03-01      128638  56857935.0         -0.002315\n",
      "2023-03-02       82293  10725142.0          0.003095\n",
      "2023-03-08       36040   4783217.0          0.002957\n",
      "2023-03-09       25597   2972829.0          0.004033\n",
      "2023-03-12       18029   3329039.0          0.000838\n",
      "2023-03-13        6313   1327746.0          0.000177\n",
      "2023-03-14       30272   4255865.0          0.002535\n",
      "2023-03-15       22666   2482509.0          0.004553\n",
      "2023-03-16       60010   7541708.0          0.003379\n",
      "2023-03-17       15416   2398226.0          0.001850\n",
      "2023-03-20       28054   4518931.0          0.001630\n",
      "2023-03-22        5122   1042333.0          0.000336\n",
      "2023-03-24       17410   3897340.0         -0.000111\n",
      "2023-03-25       32893  40734197.0         -0.003770\n",
      "2023-03-28        5680   1672133.0         -0.001181\n",
      "2023-03-31        8204   1279522.0          0.001834\n",
      "2023-04-01       58114  76779038.0         -0.003821\n",
      "2023-04-02       36051   8239177.0         -0.000202\n",
      "2023-04-03        5336   1363477.0         -0.000664\n",
      "2023-04-04        7463   2088187.0         -0.001004\n",
      "2023-04-06       39470  10748744.0         -0.000906\n",
      "2023-04-09       11848  22611267.0         -0.004054\n",
      "2023-04-12       46375  13197829.0         -0.001064\n",
      "2023-04-14        6911   1509710.0          0.000000\n",
      "2023-04-16        4075   1402847.0         -0.001673\n",
      "2023-04-18       20059   6891341.0         -0.001667\n",
      "2023-04-19        5810   1135212.0          0.000540\n",
      "2023-04-20       14378   2397758.0          0.001419\n",
      "2023-04-22       37094   7560429.0          0.000329\n",
      "2023-04-24       25082   4202409.0          0.001391\n",
      "2023-04-27       24987   7456250.0         -0.001227\n",
      "2023-04-30       21593   6631071.0         -0.001321\n",
      "2023-05-01       14751   2388399.0          0.001598\n",
      "2023-05-04       26136   4459053.0          0.001284\n",
      "2023-05-05       16004   8982257.0         -0.002796\n",
      "2023-05-08      101636  27293738.0         -0.000854\n",
      "2023-05-09       23713  19426816.0         -0.003357\n",
      "2023-05-10       30059   7728297.0         -0.000688\n",
      "2023-05-11        2189    181727.0          0.007468\n",
      "2023-05-15       10033  16436641.0         -0.003967\n",
      "2023-05-16       11448   1594576.0          0.002602\n",
      "2023-05-19       12817   1147362.0          0.006593\n",
      "DataFrame for NVDA:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2022-12-20         141     47777.0          0.000440\n",
      "2022-12-22         537    181856.0          0.000442\n",
      "2022-12-26          66     50764.0         -0.001211\n",
      "2022-12-27         104     45942.0         -0.000248\n",
      "2022-12-29         174     59265.0          0.000425\n",
      "2022-12-30          94     45968.0         -0.000466\n",
      "2022-12-31         246    115429.0         -0.000380\n",
      "2023-01-03         275    121701.0         -0.000252\n",
      "2023-01-09          76     40442.0         -0.000632\n",
      "2023-01-10          79     35833.0         -0.000307\n",
      "2023-01-11         101     39546.0          0.000043\n",
      "2023-01-12         133     45325.0          0.000423\n",
      "2023-01-20         282     83238.0          0.000877\n",
      "2023-01-31         172     64801.0          0.000143\n",
      "2023-02-01         140     37955.0          0.001177\n",
      "2023-02-06         111     38494.0          0.000372\n",
      "2023-02-07          76     33163.0         -0.000220\n",
      "2023-02-21         199     65361.0          0.000533\n",
      "2023-02-28          51     26174.0         -0.000563\n",
      "2023-03-01          82     34174.0         -0.000112\n",
      "2023-03-02         161     53906.0          0.000475\n",
      "2023-03-06          86     43144.0         -0.000518\n",
      "2023-03-07         120     40351.0          0.000463\n",
      "2023-03-13         411    103371.0          0.001465\n",
      "2023-03-15          63     38940.0         -0.000893\n",
      "2023-03-16         230    113634.0         -0.000487\n",
      "2023-03-17         153     59036.0          0.000080\n",
      "2023-03-18          99     46857.0         -0.000398\n",
      "2023-03-20         101     40456.0         -0.000015\n",
      "2023-03-21         265     97771.0          0.000199\n",
      "2023-03-23         100     43994.0         -0.000238\n",
      "2023-03-27         211     50513.0          0.001666\n",
      "2023-04-04         119     53154.0         -0.000272\n",
      "2023-04-05         101     56914.0         -0.000737\n",
      "2023-04-10          95     56311.0         -0.000824\n",
      "2023-04-12          87     41588.0         -0.000419\n",
      "2023-04-13         126     49882.0          0.000015\n",
      "2023-04-21         170     80130.0         -0.000390\n",
      "2023-04-25         106     44803.0         -0.000145\n",
      "2023-04-26         102     37578.0          0.000203\n",
      "2023-04-28          86     33788.0          0.000034\n",
      "2023-05-04          35      4402.0          0.005440\n",
      "2023-05-05          63     37553.0         -0.000834\n",
      "2023-05-10         107     37591.0          0.000335\n",
      "2023-05-11         108     39507.0          0.000222\n",
      "2023-05-12          64     38003.0         -0.000827\n",
      "2023-05-17         286     90477.0          0.000650\n",
      "2023-05-18         117     38674.0          0.000514\n",
      "DataFrame for INTC:\n",
      "            Like Count  View Count  Engagement Score\n",
      "date                                                \n",
      "2023-05-01          15      3453.0          0.001517\n",
      "2023-05-03         135     42255.0          0.000368\n",
      "2023-05-04          80     24027.0          0.000502\n",
      "2023-05-05          94     28713.0          0.000447\n",
      "2023-05-08         567    748232.0         -0.002069\n",
      "2023-05-10         492    107692.0          0.001741\n",
      "2023-05-12        1586    874986.0         -0.001015\n",
      "2023-05-15         195     85424.0         -0.000544\n",
      "2023-05-16         164    293639.0         -0.002269\n",
      "2023-05-17         216    125618.0         -0.001108\n",
      "2023-05-18         129     52449.0         -0.000368\n",
      "2023-05-19         127     22814.0          0.002740\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tweets_nb.csv')\n",
    "\n",
    "columns_to_remove = ['Tweet Id', 'Text', 'Quote Count','Vibe', 'Retweet Count', 'Conversation Id', 'Reply Count']\n",
    "\n",
    "# Drop the unnamed index column and other unecessary columns\n",
    "df = df.iloc[:, 1:]\n",
    "df = df.drop(columns_to_remove, axis=1)\n",
    "\n",
    "# Get the unique values from the first column\n",
    "unique_values = df['STOCK'].unique()\n",
    "\n",
    "# Create a dictionary to store the split DataFrames\n",
    "dfs = {}\n",
    "\n",
    "# Split the DataFrame based on unique values in the first column\n",
    "for value in unique_values:\n",
    "    dfs[value] = df[df['STOCK'] == value].copy()\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"nb_twitter_data\", exist_ok=True)\n",
    "\n",
    "# Access the split DataFrames using the unique values\n",
    "for value, split_df in dfs.items():\n",
    "    try:\n",
    "\n",
    "\n",
    "        # Remove nulls\n",
    "        dfs[value] = dfs[value].dropna()\n",
    "        \n",
    "        #group by day.\n",
    "        dfs[value]['date'] = pd.to_datetime(split_df['Datetime']).dt.date\n",
    "        dfs[value] = dfs[value].groupby('date').sum(['Like Count', 'View Count', 'Engagement Score'])\n",
    "\n",
    "        #Calculate the Engagement for any particular day.\n",
    "        dfs[value]['Engagement Score']  = dfs[value]['Like Count'] / dfs[value]['View Count']\n",
    "        dfs[value]['Engagement Score']  = dfs[value]['Engagement Score'] - dfs[value]['Engagement Score'].median()\n",
    "\n",
    "        print(f\"DataFrame for {value}:\")\n",
    "        print(dfs[value])\n",
    "\n",
    "        # Store to CSV \n",
    "        filename = (f\"{value}.csv\")\n",
    "        filepath = os.path.join(\"nb_twitter_data\", filename)\n",
    "        dfs[value].to_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing CSV file for '{value}': {str(e)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d99b34b",
   "metadata": {},
   "source": [
    "# Google Trends Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b24e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import time\n",
    "\n",
    "def predict_interest(keyword, starttime):\n",
    "    time.sleep(1)\n",
    "    prediction ={\n",
    "                 \"current_trend\": 1.0,\n",
    "                 \"predicted_trend\": 1.0,\n",
    "                 \"delta_trend\": 0}\n",
    "    \n",
    "    if keyword == 'NA':\n",
    "        return prediction\n",
    "    \n",
    "    # Set up Google Trends API\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "    \n",
    "    timeframe = starttime\n",
    "    # Query Google Trends for interest over time\n",
    "    pytrends.build_payload(kw_list=[keyword], timeframe=timeframe)\n",
    "    interest_over_time = pytrends.interest_over_time()\n",
    "    # Convert the data to a pandas DataFrame\n",
    "    df = pd.DataFrame(interest_over_time)\n",
    "    df = df.drop(df.index[-1])\n",
    "\n",
    "    return df[keyword]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56b4a009",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54a9c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import update_trends_data\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "481af019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many days in the past.\n",
    "def date_to_ints(df, col_name):\n",
    "    datetime_col = pd.to_datetime(df[col_name])\n",
    "    days_since_today = (datetime_col.dt.day)\n",
    "    return days_since_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ca122dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a datetime into the seconds of the day for training. \n",
    "def datetime_to_seconds(df, col_name):\n",
    "\n",
    "    # Extract the datetime column and convert it to datetime type if it's not already\n",
    "    datetime_col = pd.to_datetime(df[col_name])\n",
    "\n",
    "    # Convert datetime values to the number of seconds since midnight\n",
    "    seconds_since_midnight = (datetime_col.dt.hour * 3600) + (datetime_col.dt.minute * 60) + datetime_col.dt.second\n",
    "\n",
    "    return seconds_since_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72a6791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the data directory\n",
    "data_dir = './nb_stock_data/'\n",
    "\n",
    "# Get a list of all JSON files in the data directory \n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "727d2e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning LR for AAPL\n",
      "Current price: 174.94\n",
      "Predicted next closing price: 176.2078765532871\n",
      "1.0072474937309197  of my money is expected to exist after trade\n",
      "Beginning LR for AMZN\n",
      "Current price: 115.7\n",
      "Predicted next closing price: 115.02477064756432\n",
      "0.9941639641103226  of my money is expected to exist after trade\n",
      "Beginning LR for GOOGL\n",
      "Current price: 122.15\n",
      "Predicted next closing price: 123.0870941088829\n",
      "1.007671666875832  of my money is expected to exist after trade\n",
      "Beginning LR for INTC\n",
      "Current price: 29.84\n",
      "Predicted next closing price: 29.813158784989348\n",
      "0.9991004954755144  of my money is expected to exist after trade\n",
      "Beginning LR for MSFT\n",
      "Current price: 317.26999\n",
      "Predicted next closing price: 318.67473063525887\n",
      "1.0044275874792281  of my money is expected to exist after trade\n",
      "Beginning LR for NFLX\n",
      "Current price: 364.23001\n",
      "Predicted next closing price: 351.8781428136666\n",
      "0.9660877279542853  of my money is expected to exist after trade\n",
      "Beginning LR for NVDA\n",
      "Current price: 311.13\n",
      "Predicted next closing price: 315.0237369076648\n",
      "1.0125148230889494  of my money is expected to exist after trade\n",
      "Beginning LR for TSLA\n",
      "Current price: 178.7\n",
      "Predicted next closing price: 176.1982195628685\n",
      "0.9860001094732429  of my money is expected to exist after trade\n"
     ]
    }
   ],
   "source": [
    "conversion = []\n",
    "best_meta = 0\n",
    "total_conversion = 0.0\n",
    "\n",
    "# Loop through the JSON files and load the data from each file\n",
    "for json_file in json_files:\n",
    "    with open(data_dir + json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Access the 'meta' and 'values' keys in the loaded JSON data\n",
    "    meta_data = data['meta']\n",
    "    values_data = data['values']\n",
    "\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(values_data)\n",
    "    df['next_close'] = df['close'].shift(+1)\n",
    "\n",
    "    # Convert datetime column to datetime type\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    #use name of files 'AMZN,GOOGL, to calculate interest.\n",
    "    stock = json_file.split('.')[0]\n",
    "\n",
    "    #Query Google Trends for particular Stocks interest from today to last 3 months. \n",
    "    stocktrends = update_trends_data.predict_interest(keyword=stock, starttime='today 3-m')\n",
    "\n",
    "    #Load the Twitter Engagement Scores\n",
    "    filename = (f\"{stock}.csv\")\n",
    "    filepath = os.path.join(\"nb_twitter_data\", filename)\n",
    "    twitter = pd.read_csv(filepath)\n",
    "    \n",
    "    twitter['date'] = pd.to_datetime(twitter['date']).dt.date\n",
    "    df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
    "\n",
    "    length = len(df)\n",
    "    df['like_count'] = np.zeros(length)\n",
    "    df['view_count'] = np.zeros(length)\n",
    "    df['engagement'] = np.zeros(length)\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        tempval = 0\n",
    "        for t_index, t_row in twitter.iterrows():\n",
    "            t_date = t_row['date']\n",
    "            if date == t_date:\n",
    "                df.at[index, 'like_count'] = t_row['Like Count']\n",
    "                df.at[index, 'view_count'] = t_row['View Count']\n",
    "                df.at[index, 'engagement'] = t_row['Engagement Score']\n",
    "                tempval = 1\n",
    "\n",
    "        date = pd.to_datetime(date)\n",
    "        \n",
    "    # Check if the date exists in the data structure\n",
    "        if date in stocktrends:\n",
    "            df.at[index, 'value'] = stocktrends[date]\n",
    "       \n",
    "\n",
    "    # Calculate the mean of the 'value' column\n",
    "    mean_value = df['value'].mean()\n",
    "\n",
    "    # Fill missing values in the 'value' column with the mean\n",
    "    df['value'].fillna(mean_value, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Convert all other columns to numeric type\n",
    "    df[['open', 'high', 'low', 'close', 'volume', 'next_close']] = df[['open', 'high', 'low', 'close', 'volume', 'next_close']].apply(pd.to_numeric)\n",
    "    df['time'] = df['datetime'].dt.time\n",
    "    \n",
    "    df['time'] = datetime_to_seconds(df, 'datetime')\n",
    "    # Convert 'date' column to real numbers\n",
    "    df['date'] = date_to_ints(df, 'date')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print('Beginning Correlation Dependencies:')\n",
    "    #This code now adds a correlation component relating each other potential Stock to the current stock. These companies are related and there should be treated as such. \n",
    "    #Area of improvement is grabbing the pretrained datasets and then combining at the end instead of doing it here. Then we can grab values like trends and engagement.\n",
    "    for other_json in json_files:\n",
    "        if other_json == json_file:\n",
    "            #print('Skipping: ',other_json)\n",
    "            continue\n",
    "        #print('Beginning: ',other_json)\n",
    "        with open(data_dir + other_json, 'r') as f:\n",
    "            other_json_data = json.load(f)\n",
    "            other_vals = pd.DataFrame(other_json_data['values'])\n",
    "            df[other_json.split('.')[0] + '_open'] = other_vals['open'].apply(pd.to_numeric)\n",
    "            df[other_json + '_close'] = other_vals['close'].apply(pd.to_numeric)\n",
    "            \n",
    "            df[other_json.split('.')[0] + 'volume'] = other_vals['volume'].apply(pd.to_numeric)\n",
    "\n",
    "    \n",
    "\n",
    "    predicted_class= df.iloc[0]\n",
    "    df.drop(df.head(1).index, inplace=True)\n",
    "\n",
    "# Create a new dataframe with the closing prices and the next closing price\n",
    "# extract the time component\n",
    "    columns_to_drop = ['next_close', 'datetime'] # no need for datetime since we have date component. \n",
    "\n",
    "    y = df['next_close']\n",
    "    X = df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "    print('Beginning LR for', stock)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict the next closing price based on the most recent closing price\n",
    "    last_close = predicted_class.drop(columns_to_drop)\n",
    "    next_close = model.predict([last_close])\n",
    "\n",
    "    \n",
    "    print('Current price:',last_close[2] )\n",
    "    print('Predicted next closing price:',next_close[0] )\n",
    "    print((next_close[0]/last_close[2]), ' of my money is expected to exist after trade')\n",
    "\n",
    "    #Store predictions/conversions\n",
    "    total_conversion= total_conversion + (next_close[0]/last_close[2])\n",
    "    conversion.append([meta_data['symbol'], (next_close[0]/last_close[2]), last_close[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9da15b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 1.0072474937309197, 174.94]\n",
      "['AMZN', 0.9941639641103226, 115.7]\n",
      "['GOOGL', 1.007671666875832, 122.15]\n",
      "['INTC', 0.9991004954755144, 29.84]\n",
      "['MSFT', 1.0044275874792281, 317.26999]\n",
      "['NFLX', 0.9660877279542853, 364.23001]\n",
      "['NVDA', 1.0125148230889494, 311.13]\n",
      "['TSLA', 0.9860001094732429, 178.7]\n",
      "12.009075439363222\n",
      "My pie chart of assets should be:\n",
      "BUY:  AAPL 479 shares.\n",
      "SELL:  AMZN shares.\n",
      "BUY:  GOOGL 686 shares.\n",
      "SELL:  INTC shares.\n",
      "BUY:  MSFT 263 shares.\n",
      "SELL:  NFLX shares.\n",
      "BUY:  NVDA 270 shares.\n",
      "SELL:  TSLA shares.\n"
     ]
    }
   ],
   "source": [
    "for conv in conversion:\n",
    "    print(conv)\n",
    "    if conv[1] > 1.0:\n",
    "        total_conversion+= conv[1]\n",
    "print(total_conversion)\n",
    "print('My pie chart of assets should be:')\n",
    "for conv in conversion:\n",
    "    if conv[1] < 1:\n",
    "        print('SELL: ',conv[0], 'shares.')\n",
    "        continue\n",
    "    conv[1]/=total_conversion\n",
    "    print('BUY: ', conv[0], (int)(conv[1]*1000000/conv[2]), 'shares.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
